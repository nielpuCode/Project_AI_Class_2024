{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents_exercise.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    # print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data X and data Y\n",
    "words = []  \n",
    "classes = []  \n",
    "data_X = []\n",
    "data_y = []\n",
    "\n",
    "# Iterating over all the intents\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)  \n",
    "        data_X.append(pattern.lower())  # Convert pattern to lowercase\n",
    "        data_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "\n",
    "\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "# Save the variables to a Python file\n",
    "with open('variables.py', 'w') as f:\n",
    "    f.write(f'words = {words}\\n')\n",
    "    f.write(f'classes = {classes}\\n')\n",
    "    f.write(f'data_X = {data_X}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "\n",
    "# Creating the bag of words model\n",
    "for idx, doc in enumerate(data_X):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "\n",
    "    # Mark the index of class that the current pattern is associated to\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(data_y[idx])] = 1\n",
    "\n",
    "    # Add the one hot encoded BoW and associated classes to training\n",
    "    training.append([bow, output_row])\n",
    "\n",
    "# Shuffle the data and convert it to an array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "# Split the features and target labels\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_Y = np.array(list(training[:, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the Neural Network Model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_X[0]),), activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_Y[0]), activation=\"softmax\"))\n",
    "\n",
    "# Configure the optimizer\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x=train_X, y=train_Y, epochs=500, validation_split=0.2, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"chatbot_AI.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and validation metrics\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Print the last epoch's metrics\n",
    "last_epoch = len(train_loss)\n",
    "print(f\"Last Epoch: Loss={train_loss[last_epoch-1]}, Acc={train_acc[last_epoch-1]}, Val Loss={val_loss[last_epoch-1]}, Val Acc={val_acc[last_epoch-1]}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot the training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_acc) + 1), train_acc, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_acc) + 1), val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "model = load_model(\"./chatbot_AI.h5\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(text, vocab):\n",
    "    tokens = clean_text(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for word in tokens:\n",
    "        for idx, w in enumerate(vocab):\n",
    "            if w == word:\n",
    "                bow[idx] = 1\n",
    "    return np.array(bow)\n",
    "\n",
    "def pred_class(text, vocab, labels, model):\n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.5\n",
    "    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    if not intents_list:\n",
    "        return \"Sorry! I don't understand.\"\n",
    "    else:\n",
    "        tag = intents_list[0]\n",
    "        list_of_intents = intents_json[\"intents\"]\n",
    "        for i in list_of_intents:\n",
    "            if i[\"tag\"] == tag:\n",
    "                result = random.choice(i[\"responses\"])\n",
    "                break\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Press 'e' if you don't want to chat with our ChatBot.\")\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    if message == 'e':\n",
    "        break\n",
    "    intents = pred_class(message, words, classes, model)\n",
    "    result = get_response(intents, data)\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
